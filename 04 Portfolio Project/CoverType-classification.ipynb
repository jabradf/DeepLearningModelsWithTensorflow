{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is from US Forest Service (USFS) Region2: Rocky Mountain region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.metrics import confusion_matrix\\n\\nfrom tensorflow.keras.callbacks import EarlyStopping\\n\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "'''from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 55)\n",
      "Index(['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
      "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
      "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
      "       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n",
      "       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
      "       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
      "       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
      "       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
      "       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
      "       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
      "       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
      "       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
      "       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
      "       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
      "       'Soil_Type39', 'Soil_Type40', 'class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = read_csv('cover_data.csv', header=0) #, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "# summarize first few rows\n",
    "print(data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for any missing data\n",
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 2 1 7 3 6 4]\n",
      "[1 0]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "data['class'].dropna(inplace=True) # drop any na values\n",
    "print(data['class'].unique())\n",
    "#print(data['Elevation'].max())\n",
    "#print(data['Aspect'].min())\n",
    "#print(data['Slope'].max())\n",
    "#print(data['Horizontal_Distance_To_Hydrology'].min())\n",
    "#print(data['Vertical_Distance_To_Hydrology'].min())\n",
    "#print(data['Hillshade_9am'].max())\n",
    "#print(data['Hillshade_Noon'].max())\n",
    "#print(data['Hillshade_3pm'].max())\n",
    "#print(data['Horizontal_Distance_To_Fire_Points'].max())\n",
    "print(data['Wilderness_Area1'].unique())\n",
    "print(data['Wilderness_Area2'].unique())\n",
    "print(data['Wilderness_Area3'].unique())\n",
    "print(data['Wilderness_Area4'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 55 columns in the dataset, and >500k records.\n",
    "The label we will use for predictions is the 'class' column, which has 7 types, from 1-7.\n",
    "\n",
    "The following columns of interest are:\n",
    "* Elevation: in metres, with the max value of 3858\n",
    "  * we'll use 4000 as the highest value when scaling between 0-1 for the NN. \n",
    "* Aspect: in degrees. Scale from 0-360\n",
    "* Slope: In degrees. Maximum is 66. Scale from 0-70\n",
    "* Horizontal distance to Hydrology. Distance to water source. Scale from 0 to 1400\n",
    "* Vertical distance to Hydrology. Distance to water source. Scale from -180 to 650\n",
    "* Hillshade at 9am. Scale from 0 - 254\n",
    "* Hillshade at Noon. Scale from 0 - 254\n",
    "* Hillshade at 3pm. Scale from 0 - 254\n",
    "* Horizontal_Distance_To_Fire_Points. Scale from 0 - 7200\n",
    "* Wilderness_Area 1-4: Binary columns\n",
    "* Soil_Type 1-40: Binary Columns\n",
    "\n",
    "We will convert the binary columns into a single integer column for the purposes of input into the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 581012 entries, 0 to 581011\n",
      "Data columns (total 55 columns):\n",
      " #   Column                              Non-Null Count   Dtype\n",
      "---  ------                              --------------   -----\n",
      " 0   Elevation                           581012 non-null  int64\n",
      " 1   Aspect                              581012 non-null  int64\n",
      " 2   Slope                               581012 non-null  int64\n",
      " 3   Horizontal_Distance_To_Hydrology    581012 non-null  int64\n",
      " 4   Vertical_Distance_To_Hydrology      581012 non-null  int64\n",
      " 5   Horizontal_Distance_To_Roadways     581012 non-null  int64\n",
      " 6   Hillshade_9am                       581012 non-null  int64\n",
      " 7   Hillshade_Noon                      581012 non-null  int64\n",
      " 8   Hillshade_3pm                       581012 non-null  int64\n",
      " 9   Horizontal_Distance_To_Fire_Points  581012 non-null  int64\n",
      " 10  Wilderness_Area1                    581012 non-null  int64\n",
      " 11  Wilderness_Area2                    581012 non-null  int64\n",
      " 12  Wilderness_Area3                    581012 non-null  int64\n",
      " 13  Wilderness_Area4                    581012 non-null  int64\n",
      " 14  Soil_Type1                          581012 non-null  int64\n",
      " 15  Soil_Type2                          581012 non-null  int64\n",
      " 16  Soil_Type3                          581012 non-null  int64\n",
      " 17  Soil_Type4                          581012 non-null  int64\n",
      " 18  Soil_Type5                          581012 non-null  int64\n",
      " 19  Soil_Type6                          581012 non-null  int64\n",
      " 20  Soil_Type7                          581012 non-null  int64\n",
      " 21  Soil_Type8                          581012 non-null  int64\n",
      " 22  Soil_Type9                          581012 non-null  int64\n",
      " 23  Soil_Type10                         581012 non-null  int64\n",
      " 24  Soil_Type11                         581012 non-null  int64\n",
      " 25  Soil_Type12                         581012 non-null  int64\n",
      " 26  Soil_Type13                         581012 non-null  int64\n",
      " 27  Soil_Type14                         581012 non-null  int64\n",
      " 28  Soil_Type15                         581012 non-null  int64\n",
      " 29  Soil_Type16                         581012 non-null  int64\n",
      " 30  Soil_Type17                         581012 non-null  int64\n",
      " 31  Soil_Type18                         581012 non-null  int64\n",
      " 32  Soil_Type19                         581012 non-null  int64\n",
      " 33  Soil_Type20                         581012 non-null  int64\n",
      " 34  Soil_Type21                         581012 non-null  int64\n",
      " 35  Soil_Type22                         581012 non-null  int64\n",
      " 36  Soil_Type23                         581012 non-null  int64\n",
      " 37  Soil_Type24                         581012 non-null  int64\n",
      " 38  Soil_Type25                         581012 non-null  int64\n",
      " 39  Soil_Type26                         581012 non-null  int64\n",
      " 40  Soil_Type27                         581012 non-null  int64\n",
      " 41  Soil_Type28                         581012 non-null  int64\n",
      " 42  Soil_Type29                         581012 non-null  int64\n",
      " 43  Soil_Type30                         581012 non-null  int64\n",
      " 44  Soil_Type31                         581012 non-null  int64\n",
      " 45  Soil_Type32                         581012 non-null  int64\n",
      " 46  Soil_Type33                         581012 non-null  int64\n",
      " 47  Soil_Type34                         581012 non-null  int64\n",
      " 48  Soil_Type35                         581012 non-null  int64\n",
      " 49  Soil_Type36                         581012 non-null  int64\n",
      " 50  Soil_Type37                         581012 non-null  int64\n",
      " 51  Soil_Type38                         581012 non-null  int64\n",
      " 52  Soil_Type39                         581012 non-null  int64\n",
      " 53  Soil_Type40                         581012 non-null  int64\n",
      " 54  class                               581012 non-null  int64\n",
      "dtypes: int64(55)\n",
      "memory usage: 243.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no blank columns so we should be ok to not have to remove unexpected data.\n",
    "\n",
    "# Build the Model\n",
    "Here we will use a sequential mode, with a Dense layer with 7 outputs to choose categorial labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wilderness_Area1' 'Wilderness_Area3' 'Wilderness_Area4'\n",
      " 'Wilderness_Area2']\n"
     ]
    }
   ],
   "source": [
    "# Reverse the one-hot encoded columns for the wilderness area columns\n",
    "data['Wilderness_Area'] = data.loc[:, 'Wilderness_Area1':'Wilderness_Area4'].idxmax(axis=1)\n",
    "print(data['Wilderness_Area'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Soil_Type29' 'Soil_Type12' 'Soil_Type30' 'Soil_Type18' 'Soil_Type16'\n",
      " 'Soil_Type20' 'Soil_Type24' 'Soil_Type23' 'Soil_Type40' 'Soil_Type19'\n",
      " 'Soil_Type8' 'Soil_Type22' 'Soil_Type39' 'Soil_Type9' 'Soil_Type38'\n",
      " 'Soil_Type33' 'Soil_Type31' 'Soil_Type32' 'Soil_Type11' 'Soil_Type10'\n",
      " 'Soil_Type5' 'Soil_Type28' 'Soil_Type4' 'Soil_Type1' 'Soil_Type13'\n",
      " 'Soil_Type2' 'Soil_Type17' 'Soil_Type3' 'Soil_Type34' 'Soil_Type6'\n",
      " 'Soil_Type14' 'Soil_Type37' 'Soil_Type35' 'Soil_Type36' 'Soil_Type21'\n",
      " 'Soil_Type26' 'Soil_Type27' 'Soil_Type25' 'Soil_Type7' 'Soil_Type15']\n"
     ]
    }
   ],
   "source": [
    "# Reverse the one-hot encoded columns for the Soil Type columns\n",
    "data['Soil_Type'] = data.loc[:, 'Soil_Type1':'Soil_Type40'].idxmax(axis=1)\n",
    "print(data['Soil_Type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[f\"Wilderness_Area{i}\" for i in range(1, 5)])\n",
    "data = data.drop(columns=[f\"Soil_Type{i}\" for i in range(1, 41)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "# Change the datatypes of the new column to integer\n",
    "data['Wilderness_Area'] = data['Wilderness_Area'].str.extract(r'(\\d+)').astype(int)\n",
    "data['Soil_Type'] = data['Soil_Type'].str.extract(r'(\\d+)').astype(int)\n",
    "print(data['Wilderness_Area'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class column needs to be 0-6, not 1-7.\n",
    "data['class'] = data['class'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "First step is to split the data into train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and labels\n",
    "X = data.drop(columns=['class'])\n",
    "y = data['class']\n",
    "\n",
    "# normalise the feature set\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# split the features and labels into training,test, and validation sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=40)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14526/14526 [==============================] - 27s 2ms/step - loss: 0.6491 - accuracy: 0.7278 - val_loss: 0.5986 - val_accuracy: 0.7472\n",
      "Epoch 2/20\n",
      "14526/14526 [==============================] - 25s 2ms/step - loss: 0.5918 - accuracy: 0.7493 - val_loss: 0.5798 - val_accuracy: 0.7549\n",
      "Epoch 3/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.5748 - accuracy: 0.7569 - val_loss: 0.5655 - val_accuracy: 0.7588\n",
      "Epoch 4/20\n",
      "14526/14526 [==============================] - 28s 2ms/step - loss: 0.5645 - accuracy: 0.7606 - val_loss: 0.5591 - val_accuracy: 0.7615\n",
      "Epoch 5/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.5586 - accuracy: 0.7632 - val_loss: 0.5575 - val_accuracy: 0.7655\n",
      "Epoch 6/20\n",
      "14526/14526 [==============================] - 28s 2ms/step - loss: 0.5544 - accuracy: 0.7654 - val_loss: 0.5510 - val_accuracy: 0.7669\n",
      "Epoch 7/20\n",
      "14526/14526 [==============================] - 28s 2ms/step - loss: 0.5514 - accuracy: 0.7667 - val_loss: 0.5506 - val_accuracy: 0.7670\n",
      "Epoch 8/20\n",
      "14526/14526 [==============================] - 28s 2ms/step - loss: 0.5489 - accuracy: 0.7675 - val_loss: 0.5464 - val_accuracy: 0.7666\n",
      "Epoch 9/20\n",
      "14526/14526 [==============================] - 29s 2ms/step - loss: 0.5464 - accuracy: 0.7687 - val_loss: 0.5462 - val_accuracy: 0.7670\n",
      "Epoch 10/20\n",
      "14526/14526 [==============================] - 29s 2ms/step - loss: 0.5445 - accuracy: 0.7692 - val_loss: 0.5440 - val_accuracy: 0.7687\n",
      "Epoch 11/20\n",
      "14526/14526 [==============================] - 28s 2ms/step - loss: 0.5429 - accuracy: 0.7691 - val_loss: 0.5393 - val_accuracy: 0.7714\n",
      "Epoch 12/20\n",
      "14526/14526 [==============================] - 29s 2ms/step - loss: 0.5417 - accuracy: 0.7695 - val_loss: 0.5392 - val_accuracy: 0.7715\n",
      "Epoch 13/20\n",
      "14526/14526 [==============================] - 29s 2ms/step - loss: 0.5409 - accuracy: 0.7696 - val_loss: 0.5402 - val_accuracy: 0.7717\n",
      "Epoch 14/20\n",
      "14526/14526 [==============================] - 28s 2ms/step - loss: 0.5399 - accuracy: 0.7702 - val_loss: 0.5372 - val_accuracy: 0.7696\n",
      "Epoch 15/20\n",
      "14526/14526 [==============================] - 29s 2ms/step - loss: 0.5394 - accuracy: 0.7698 - val_loss: 0.5385 - val_accuracy: 0.7705\n",
      "Epoch 16/20\n",
      "14526/14526 [==============================] - 28s 2ms/step - loss: 0.5389 - accuracy: 0.7696 - val_loss: 0.5354 - val_accuracy: 0.7708\n",
      "Epoch 17/20\n",
      "14526/14526 [==============================] - 27s 2ms/step - loss: 0.5383 - accuracy: 0.7705 - val_loss: 0.5401 - val_accuracy: 0.7710\n",
      "Epoch 18/20\n",
      "14526/14526 [==============================] - 24s 2ms/step - loss: 0.5378 - accuracy: 0.7708 - val_loss: 0.5334 - val_accuracy: 0.7714\n",
      "Epoch 19/20\n",
      "14526/14526 [==============================] - 25s 2ms/step - loss: 0.5371 - accuracy: 0.7712 - val_loss: 0.5344 - val_accuracy: 0.7729\n",
      "Epoch 20/20\n",
      "14526/14526 [==============================] - 25s 2ms/step - loss: 0.5368 - accuracy: 0.7717 - val_loss: 0.5327 - val_accuracy: 0.7732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23dd0367490>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(X_train.shape[1],))) # automatically select the number of inputs based on the shape of the data\n",
    "\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "# output layer, 7+1 possible outcomes\n",
    "model.add(tf.keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial mode is ok for a first attempt, at around 77% accuracy.\n",
    "\n",
    "We need to optimise the model by adjusting the following parameters:\n",
    "* batch size\n",
    "* optimiser types\n",
    "* learning rate\n",
    "* hidden units\n",
    "* number of layers\n",
    "* implementation of dropout and early stopping\n",
    "* activation functions\n",
    "\n",
    "Using a Keras tuner can take care of a lot of the tuning for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 09m 18s]\n",
      "val_accuracy: 0.7912772297859192\n",
      "\n",
      "Best val_accuracy So Far: 0.7983339428901672\n",
      "Total elapsed time: 00h 46m 36s\n",
      "Epoch 1/20\n",
      "14526/14526 [==============================] - 27s 2ms/step - loss: 0.6244 - accuracy: 0.7385 - val_loss: 0.5753 - val_accuracy: 0.7552\n",
      "Epoch 2/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.5623 - accuracy: 0.7606 - val_loss: 0.5469 - val_accuracy: 0.7677\n",
      "Epoch 3/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.5434 - accuracy: 0.7684 - val_loss: 0.5387 - val_accuracy: 0.7711\n",
      "Epoch 4/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.5319 - accuracy: 0.7737 - val_loss: 0.5235 - val_accuracy: 0.7780\n",
      "Epoch 5/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.5230 - accuracy: 0.7780 - val_loss: 0.5146 - val_accuracy: 0.7828\n",
      "Epoch 6/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.5154 - accuracy: 0.7819 - val_loss: 0.5106 - val_accuracy: 0.7842\n",
      "Epoch 7/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.5082 - accuracy: 0.7857 - val_loss: 0.5007 - val_accuracy: 0.7879\n",
      "Epoch 8/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.5025 - accuracy: 0.7877 - val_loss: 0.4968 - val_accuracy: 0.7891\n",
      "Epoch 9/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4978 - accuracy: 0.7897 - val_loss: 0.5050 - val_accuracy: 0.7845\n",
      "Epoch 10/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4943 - accuracy: 0.7915 - val_loss: 0.4877 - val_accuracy: 0.7954\n",
      "Epoch 11/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4912 - accuracy: 0.7931 - val_loss: 0.4882 - val_accuracy: 0.7928\n",
      "Epoch 12/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4883 - accuracy: 0.7941 - val_loss: 0.4850 - val_accuracy: 0.7950\n",
      "Epoch 13/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4860 - accuracy: 0.7949 - val_loss: 0.4922 - val_accuracy: 0.7885\n",
      "Epoch 14/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4844 - accuracy: 0.7955 - val_loss: 0.4795 - val_accuracy: 0.8002\n",
      "Epoch 15/20\n",
      "14526/14526 [==============================] - 27s 2ms/step - loss: 0.4826 - accuracy: 0.7965 - val_loss: 0.4793 - val_accuracy: 0.7963\n",
      "Epoch 16/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4807 - accuracy: 0.7971 - val_loss: 0.4757 - val_accuracy: 0.7974\n",
      "Epoch 17/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4797 - accuracy: 0.7973 - val_loss: 0.4789 - val_accuracy: 0.7960\n",
      "Epoch 18/20\n",
      "14526/14526 [==============================] - 27s 2ms/step - loss: 0.4782 - accuracy: 0.7983 - val_loss: 0.4744 - val_accuracy: 0.7992\n",
      "Epoch 19/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4771 - accuracy: 0.7984 - val_loss: 0.4704 - val_accuracy: 0.8026\n",
      "Epoch 20/20\n",
      "14526/14526 [==============================] - 26s 2ms/step - loss: 0.4759 - accuracy: 0.7987 - val_loss: 0.4695 - val_accuracy: 0.8004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23e0b501410>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function that takes in the hyperparameter object, hp\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    model.add(layers.Dense(hp.Choice('units', [32, 64, 128, 256]), activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(build_model, objective='val_accuracy', max_trials=5, overwrite=True, directory='my_tuner_directory')\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "best_model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy with this basic tuning didn't really improve things much; we're at 80% but can do better. Let's try adding some more layers and see how that improves things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 40m 42s]\n",
      "val_accuracy: 0.9449235200881958\n",
      "\n",
      "Best val_accuracy So Far: 0.9449235200881958\n",
      "Total elapsed time: 02h 34m 59s\n",
      "Epoch 1/20\n",
      "14526/14526 [==============================] - 120s 8ms/step - loss: 0.5232 - accuracy: 0.7791 - val_loss: 0.4215 - val_accuracy: 0.8207\n",
      "Epoch 2/20\n",
      "14526/14526 [==============================] - 120s 8ms/step - loss: 0.3740 - accuracy: 0.8437 - val_loss: 0.3408 - val_accuracy: 0.8583\n",
      "Epoch 3/20\n",
      "14526/14526 [==============================] - 120s 8ms/step - loss: 0.3087 - accuracy: 0.8732 - val_loss: 0.2945 - val_accuracy: 0.8796\n",
      "Epoch 4/20\n",
      "14526/14526 [==============================] - 119s 8ms/step - loss: 0.2660 - accuracy: 0.8912 - val_loss: 0.2558 - val_accuracy: 0.8959\n",
      "Epoch 5/20\n",
      "14526/14526 [==============================] - 120s 8ms/step - loss: 0.2376 - accuracy: 0.9032 - val_loss: 0.2290 - val_accuracy: 0.9079\n",
      "Epoch 6/20\n",
      "14526/14526 [==============================] - 121s 8ms/step - loss: 0.2158 - accuracy: 0.9123 - val_loss: 0.2150 - val_accuracy: 0.9129\n",
      "Epoch 7/20\n",
      "14526/14526 [==============================] - 121s 8ms/step - loss: 0.1989 - accuracy: 0.9194 - val_loss: 0.2036 - val_accuracy: 0.9181\n",
      "Epoch 8/20\n",
      "14526/14526 [==============================] - 121s 8ms/step - loss: 0.1857 - accuracy: 0.9247 - val_loss: 0.1925 - val_accuracy: 0.9223\n",
      "Epoch 9/20\n",
      "14526/14526 [==============================] - 121s 8ms/step - loss: 0.1741 - accuracy: 0.9295 - val_loss: 0.1870 - val_accuracy: 0.9243\n",
      "Epoch 10/20\n",
      "14526/14526 [==============================] - 122s 8ms/step - loss: 0.1649 - accuracy: 0.9334 - val_loss: 0.1713 - val_accuracy: 0.9318\n",
      "Epoch 11/20\n",
      "14526/14526 [==============================] - 122s 8ms/step - loss: 0.1569 - accuracy: 0.9361 - val_loss: 0.1737 - val_accuracy: 0.9299\n",
      "Epoch 12/20\n",
      "14526/14526 [==============================] - 122s 8ms/step - loss: 0.1498 - accuracy: 0.9393 - val_loss: 0.1674 - val_accuracy: 0.9333\n",
      "Epoch 13/20\n",
      "14526/14526 [==============================] - 122s 8ms/step - loss: 0.1435 - accuracy: 0.9417 - val_loss: 0.1673 - val_accuracy: 0.9333\n",
      "Epoch 14/20\n",
      "14526/14526 [==============================] - 122s 8ms/step - loss: 0.1373 - accuracy: 0.9442 - val_loss: 0.1585 - val_accuracy: 0.9367\n",
      "Epoch 15/20\n",
      "14526/14526 [==============================] - 122s 8ms/step - loss: 0.1327 - accuracy: 0.9460 - val_loss: 0.1478 - val_accuracy: 0.9408\n",
      "Epoch 16/20\n",
      "14526/14526 [==============================] - 123s 8ms/step - loss: 0.1281 - accuracy: 0.9479 - val_loss: 0.1464 - val_accuracy: 0.9410\n",
      "Epoch 17/20\n",
      "14526/14526 [==============================] - 123s 8ms/step - loss: 0.1232 - accuracy: 0.9500 - val_loss: 0.1507 - val_accuracy: 0.9398\n",
      "Epoch 18/20\n",
      "14526/14526 [==============================] - 123s 8ms/step - loss: 0.1193 - accuracy: 0.9516 - val_loss: 0.1520 - val_accuracy: 0.9395\n",
      "Epoch 19/20\n",
      "14526/14526 [==============================] - 123s 8ms/step - loss: 0.1155 - accuracy: 0.9531 - val_loss: 0.1428 - val_accuracy: 0.9425\n",
      "Epoch 20/20\n",
      "14526/14526 [==============================] - 125s 9ms/step - loss: 0.1126 - accuracy: 0.9540 - val_loss: 0.1311 - val_accuracy: 0.9482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23e25d886d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function that takes in the hyperparameter object, hp\n",
    "def build_model2(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    layerNum = hp.Int('layers', min_value=2, max_value=7, step=1)\n",
    "    for _ in range(layerNum):\n",
    "        model.add(layers.Dense(units=units, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-4, 1e-5])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuned2 = RandomSearch(build_model2, objective='val_accuracy', max_trials=5, overwrite=True, directory='my_tuner_directory')\n",
    "\n",
    "tuned2.search(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
    "best_hps = tuned2.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "best_model2 = tuned2.hypermodel.build(best_hps)\n",
    "best_model2.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 352, 'layers': 6, 'learning_rate': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improved things a lot, now with an accuracy of 94%!\n",
    "\n",
    "## Test the model\n",
    "We'll now evaluate the model using the test data that was set aside.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816/1816 - 6s - loss: 0.1351 - accuracy: 0.9464 - 6s/epoch - 3ms/step\n",
      "Test accuracy: 0.9464\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = best_model2.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy seems to be similar to the rates previously shown. This could be misleading however, so let's see what the representation of the labels in the dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    283301\n",
      "1    211840\n",
      "3     35754\n",
      "7     20510\n",
      "6     17367\n",
      "5      9493\n",
      "4      2747\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "temp_data = data['class'] +1 \n",
    "print(temp_data.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is heavily skewed towards classes of 2 and 1, so the model will be more accurate when fed corresponding information that relate to these classes, and less likely to predict classes of 4-6.\n",
    "\n",
    "The training data could be normalised to present a more balanced training set so that the classes with fewer labels get a fair chance of being correctly labelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "This model can be saved as a `keras` file for deployment elsewhere, such as in a cloud service, or on an edge device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model2.save(\"./coverage_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
